{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bb2172",
   "metadata": {},
   "source": [
    "** Load data/partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a9042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Training Partitons\n",
    "data_dir = \"../data/raw/Cleaned SWANSF Dataset/Cleaned SWANSF Dataset/train/\"\n",
    "X_train = []\n",
    "y_train = []\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(num_partitions):\n",
    "    with open(f\"{data_dir}Partition{i+1}_LSBZM-Norm_FPCKNN-impute.pkl\", 'rb') as f:\n",
    "        X_train.append(pickle.load(f))\n",
    "    with open(f\"{data_dir}Partition{i+1}_Labels_LSBZM-Norm_FPCKNN-impute.pkl \", 'rb') as f:\n",
    "        y_train.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba1844f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Test Partitons\n",
    "data_dir = \"../data/raw/Cleaned SWANSF Dataset/Cleaned SWANSF Dataset/test/\"\n",
    "X_test = []\n",
    "y_test = []\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(num_partitions):\n",
    "    with open(f\"{data_dir}Partition{i+1}_RUS-Tomek-TimeGAN_LSBZM-Norm_WithoutC_FPCKNN-impute.pkl\", 'rb') as f:\n",
    "        X_test.append(pickle.load(f))\n",
    "    with open(f\"{data_dir}Partition{i+1}_Labels_RUS-Tomek-TimeGAN_LSBZM-Norm_WithoutC_FPCKNN-impute.pkl\", 'rb') as f:\n",
    "        y_test.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23dbe67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Partition 1: (73492, 60, 24), Labels: (73492,)\n",
      "Train Partition 2: (88557, 60, 24), Labels: (88557,)\n",
      "Train Partition 3: (42510, 60, 24), Labels: (42510,)\n",
      "Train Partition 4: (51261, 60, 24), Labels: (51261,)\n",
      "Train Partition 5: (75365, 60, 24), Labels: (75365,)\n",
      "Test Partition 1: (18773, 60, 24), Labels: (18773,)\n",
      "Test Partition 2: (19807, 60, 24), Labels: (19807,)\n",
      "Test Partition 3: (19965, 60, 24), Labels: (19965,)\n",
      "Test Partition 4: (19320, 60, 24), Labels: (19320,)\n",
      "Test Partition 5: (19899, 60, 24), Labels: (19899,)\n",
      "Partition 1 training labels sample: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Partition 2 training labels sample: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Partition 3 training labels sample: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Partition 4 training labels sample: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Partition 5 training labels sample: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "# shape check\n",
    "for i, part in enumerate(X_train):\n",
    "    print(f\"Train Partition {i+1}: {part.shape}, Labels: {y_train[i].shape}\")\n",
    "for i, part in enumerate(X_test):\n",
    "    print(f\"Test Partition {i+1}: {part.shape}, Labels: {y_test[i].shape}\")\n",
    "    \n",
    "for i in range(5):\n",
    "    print(f\"Partition {i+1} training labels sample:\", y_train[i][:10])\n",
    "\n",
    "\n",
    "print(type(y_train[0][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "488dbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all partitions into single arrays\n",
    "\n",
    "X_train_full = np.concatenate(X_train, axis=0)\n",
    "y_train_full = np.concatenate(y_train, axis=0)\n",
    "X_test_full  = np.concatenate(X_test, axis=0)\n",
    "y_test_full  = np.concatenate(y_test, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f202c",
   "metadata": {},
   "source": [
    "**Merge or Select Partition Pairs Dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e086dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Sequential and proposed pairings\n",
    "pairs = [(0,1), (1,2), (2,3), (3,4), (0,2), (0,3), (0,4)]\n",
    "\n",
    "def get_partition_pair(train_idx, test_idx):\n",
    "    \"\"\"\n",
    "    Selects a partition pair and filters for M vs X classification.\n",
    "    \"\"\"\n",
    "    X_train_pair = np.array(X_train[train_idx])\n",
    "    y_train_pair = np.array(y_train[train_idx])\n",
    "    X_test_pair  = np.array(X_test[test_idx])\n",
    "    y_test_pair  = np.array(y_test[test_idx])\n",
    "\n",
    "    # Apply filtering\n",
    "    X_train_pair, y_train_pair = filter_mx(X_train_pair, y_train_pair)\n",
    "    X_test_pair,  y_test_pair  = filter_mx(X_test_pair,  y_test_pair)\n",
    "\n",
    "    print(f\"Train P{train_idx+1}: {X_train_pair.shape}, Labels: {np.unique(y_train_pair, return_counts=True)}\")\n",
    "    print(f\"Test  P{test_idx+1}: {X_test_pair.shape},  Labels: {np.unique(y_test_pair, return_counts=True)}\\n\")\n",
    "\n",
    "    return X_train_pair, y_train_pair, X_test_pair, y_test_pair\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccdaef3",
   "metadata": {},
   "source": [
    "**Preprocessing for M vs X Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43fb6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_partition_pair(train_idx, test_idx, shuffle=False, seed=42):\n",
    "    \"\"\"\n",
    "    Selects a train/test partition pair and prepares it for modeling.\n",
    "\n",
    "    Args:\n",
    "        train_idx (int): index (0-4) of training partition\n",
    "        test_idx  (int): index (0-4) of testing partition\n",
    "        shuffle   (bool): whether to shuffle training samples\n",
    "        seed      (int): random seed for reproducible shuffling\n",
    "\n",
    "    Returns:\n",
    "        X_train_pair (np.ndarray): (n_train, seq_len, n_features)\n",
    "        y_train_pair (np.ndarray): (n_train,)\n",
    "        X_test_pair  (np.ndarray): (n_test,  seq_len, n_features)\n",
    "        y_test_pair  (np.ndarray): (n_test,)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Load selected partitions ---\n",
    "    X_train_pair = np.array(X_train[train_idx], dtype=np.float32)\n",
    "    y_train_pair = np.array(y_train[train_idx]).astype(int)\n",
    "    X_test_pair  = np.array(X_test[test_idx],  dtype=np.float32)\n",
    "    y_test_pair  = np.array(y_test[test_idx]).astype(int)\n",
    "\n",
    "    # --- Optional shuffle for “out-of-order” experiment ---\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.permutation(len(X_train_pair))\n",
    "        X_train_pair, y_train_pair = X_train_pair[idx], y_train_pair[idx]\n",
    "\n",
    "    # --- Diagnostics ---\n",
    "    print(f\"Train P{train_idx+1}: {X_train_pair.shape}, \"\n",
    "          f\"label counts: {np.bincount(y_train_pair)}\")\n",
    "    print(f\"Test  P{test_idx+1}: {X_test_pair.shape},  \"\n",
    "          f\"label counts: {np.bincount(y_test_pair)}\\n\")\n",
    "\n",
    "    return X_train_pair, y_train_pair, X_test_pair, y_test_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f345a1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train P1: (73492, 60, 24), label counts: [72238  1254]\n",
      "Test  P2: (19807, 60, 24),  label counts: [10000  9807]\n",
      "\n",
      "Train P1: (73492, 60, 24), label counts: [72238  1254]\n",
      "Test  P2: (19807, 60, 24),  label counts: [10000  9807]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sequential pair example: P1 → P2\n",
    "Xtr, ytr, Xte, yte = get_partition_pair(0, 1)\n",
    "\n",
    "# Optional shuffled training\n",
    "Xtr_shuf, ytr_shuf, _, _ = get_partition_pair(0, 1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4a621",
   "metadata": {},
   "source": [
    "**Convert NumPy arrays to a Hugging Face DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77e0123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_hf_dataset(Xtr, ytr, Xte, yte):\n",
    "    \"\"\"\n",
    "    Converts NumPy arrays into Hugging Face DatasetDict.\n",
    "    Each element:\n",
    "        past_values -> (seq_len, n_features) array\n",
    "        labels -> scalar 0 or 1\n",
    "    \"\"\"\n",
    "    train_dict = {\n",
    "        \"past_values\": [x for x in Xtr],   # list of 2-D arrays\n",
    "        \"labels\":      [int(y) for y in ytr]\n",
    "    }\n",
    "\n",
    "    test_dict = {\n",
    "        \"past_values\": [x for x in Xte],\n",
    "        \"labels\":      [int(y) for y in yte]\n",
    "    }\n",
    "\n",
    "    train_dataset = Dataset.from_dict(train_dict)\n",
    "    test_dataset  = Dataset.from_dict(test_dict)\n",
    "\n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\":  test_dataset\n",
    "    })\n",
    "\n",
    "    print(dataset_dict)\n",
    "    return dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4539d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['past_values', 'labels'],\n",
      "        num_rows: 73492\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['past_values', 'labels'],\n",
      "        num_rows: 19807\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = build_hf_dataset(Xtr, ytr, Xte, yte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58056ef",
   "metadata": {},
   "source": [
    "** Model Configuration and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "458bcc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerConfig\n",
    "\n",
    "config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=1,\n",
    "    lags_sequence=[0],\n",
    "    context_length=60,     # your timesteps\n",
    "    input_size=24,         # your features\n",
    "    num_time_features=0,\n",
    "    num_static_categorical_features=0,\n",
    "    d_model=128,\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=0,\n",
    "    dropout=0.1,\n",
    "    is_encoder_decoder=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0ae0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\UCCS_Projects\\solar-flare-project\\.SoFlareEnv\\Scripts\\python.exe\n",
      "c:\\UCCS_Projects\\solar-flare-project\\.SoFlareEnv\\Lib\\site-packages\\transformers\\__init__.py\n",
      "4.57.1\n",
      "Torch: 2.9.0+cpu\n",
      "TorchVision: 0.24.0+cpu\n",
      "Transformers: 4.57.1\n",
      "TimeSeriesTransformerModel imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys, transformers\n",
    "print(sys.executable)\n",
    "print(transformers.__file__)\n",
    "print(transformers.__version__)\n",
    "\n",
    "import torch, torchvision, transformers\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"TorchVision:\", torchvision.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "\n",
    "from transformers import TimeSeriesTransformerModel\n",
    "print(\"TimeSeriesTransformerModel imported successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87885f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class TimeSeriesTransformerForClassification(nn.Module):\n",
    "    def __init__(self, config, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = TimeSeriesTransformerModel(config)\n",
    "        self.fc = nn.Linear(config.d_model, num_classes)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, past_values, labels=None):\n",
    "        batch_size, seq_len, n_features = past_values.shape\n",
    "        \n",
    "\n",
    "        # Dummy time features (unused)\n",
    "        past_time_features = torch.zeros(\n",
    "            (batch_size, seq_len, 0),\n",
    "            dtype=past_values.dtype,\n",
    "            device=past_values.device,\n",
    "        )\n",
    "\n",
    "        # Mask: every time step and feature observed\n",
    "        past_observed_mask = torch.ones(\n",
    "            (batch_size, seq_len, n_features),\n",
    "            dtype=torch.bool,\n",
    "            device=past_values.device,\n",
    "        )\n",
    "\n",
    "        outputs = self.backbone(\n",
    "            past_values=past_values,\n",
    "            past_time_features=past_time_features,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "        )\n",
    "\n",
    "        # Take final hidden state\n",
    "        last_hidden = outputs.last_hidden_state[:, -1, :]\n",
    "        logits = self.fc(last_hidden)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6036cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSeriesTransformerForClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70575b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward OK — loss: 0.6303568482398987  logits: torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "sample_batch = [dataset_dict[\"train\"][i] for i in range(4)]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X = torch.tensor([item[\"past_values\"] for item in batch], dtype=torch.float32)\n",
    "    y = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n",
    "    return {\"past_values\": X, \"labels\": y}\n",
    "\n",
    "batch = collate_fn(sample_batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(batch[\"past_values\"], batch[\"labels\"])\n",
    "print(\"Forward OK — loss:\", outputs[\"loss\"].item(),\n",
    "      \" logits:\", outputs[\"logits\"].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3a426",
   "metadata": {},
   "source": [
    "** Train the Model and Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca089a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load accuracy & recall from Hugging Face evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    # Basic metrics\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    rec = recall_metric.compute(predictions=preds, references=labels)[\"recall\"]\n",
    "\n",
    "    # Compute True Skill Statistic (TSS)\n",
    "    TP = np.sum((preds == 1) & (labels == 1))\n",
    "    TN = np.sum((preds == 0) & (labels == 0))\n",
    "    FP = np.sum((preds == 1) & (labels == 0))\n",
    "    FN = np.sum((preds == 0) & (labels == 1))\n",
    "    TSS = (TP / (TP + FN + 1e-8)) - (FP / (FP + TN + 1e-8))\n",
    "\n",
    "    return {\"accuracy\": acc, \"recall\": rec, \"tss\": TSS}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27657e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\UCCS_Projects\\solar-flare-project\\.SoFlareEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18374' max='18374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18374/18374 11:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Tss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>2.357429</td>\n",
       "      <td>0.504872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.149200</td>\n",
       "      <td>2.328117</td>\n",
       "      <td>0.504872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\UCCS_Projects\\solar-flare-project\\.SoFlareEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18374, training_loss=0.09048609515197703, metrics={'train_runtime': 680.362, 'train_samples_per_second': 216.038, 'train_steps_per_second': 27.006, 'total_flos': 0.0, 'train_loss': 0.09048609515197703, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-4,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".SoFlareEnv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
